{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Vosk\n",
    "\n",
    "Vosk is a speech recongition toolkit, the offical website is [here](https://alphacephei.com/vosk/).\n",
    "\n",
    "We choose it from the following advantages of it:\n",
    "\n",
    "1. Works offline, even on lightweight devices - Just like our Raspberry Pi\n",
    "2. Portable per-language models are only 50Mb each, but there are much bigger server models available.\n",
    "3. Supports speaker identification beside simple speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# set project root\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/thaonan/Projects/Dormitory-Assistant\")\n",
    "\n",
    "%cd {PROJECT_ROOT}\n",
    "\n",
    "# using following code to download vosk\n",
    "\n",
    "# Install required packages\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y portaudio19-dev\n",
    "!sudo apt install ffmpeg  # For audio conversion\n",
    "\n",
    "%pip install vosk\n",
    "%pip install --force-reinstall sounddevice\n",
    "%pip install pydub\n",
    "%pip install scipy\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install soundfile\n",
    "\n",
    "\n",
    "import time\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Command line Use examples\n",
    "You can transcribe a file with a simple vosk-transcriber command line tool. \n",
    "\n",
    "### 1. Video2Txt / Audio2Txt\n",
    "\n",
    "```sh\n",
    "vosk-transcriber -i test.mp4 -o test.txt\n",
    "vosk-transcriber -i test.mp4 -t srt -o test.srt\n",
    "vosk-transcriber -l fr -i test.m4a -t srt -o test.srt\n",
    "vosk-transcriber --list-languages\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Transfer mp4 to txt (set language as en)\n",
    "!vosk-transcriber -i resource/test.mp4 -o resource/test.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see all  all the supporting languages\n",
    "!vosk-transcriber --list-languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearker Identification Inplementation\n",
    "\n",
    "Now lets start to try its speaker identification functions.\n",
    "\n",
    "#### 1. Importing Required Modules:\n",
    "\n",
    "- os, sys → Handle file paths and system operations.\n",
    "- wave → Read WAV audio files.\n",
    "- json → Parse recognition results.\n",
    "- numpy → Perform numerical operations (for speaker verification).\n",
    "- vosk → The Vosk speech recognition engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import wave\n",
    "import json\n",
    "import numpy as np\n",
    "from vosk import Model, KaldiRecognizer, SpkModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Download and Check Speaker Model\n",
    "\n",
    "Now we are going to download a speaker model now. The download page is [here](https://alphacephei.com/vosk/models), there are multiple speaker model you can use.\n",
    "The speaker model is used to extract **speaker embeddings** (also called \"x-vectors\") from the audio input, and also well be used to identify the speaker. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPK_MODEL_PATH = \"model-spk\"\n",
    "\n",
    "# Download and extract the SPEAKER model (not the small-en-us model)\n",
    "!wget https://alphacephei.com/vosk/models/vosk-model-spk-0.4.zip\n",
    "!unzip vosk-model-spk-0.4.zip -d {SPK_MODEL_PATH}\n",
    "!ls {SPK_MODEL_PATH}/\n",
    "\n",
    "\n",
    "if not os.path.exists(SPK_MODEL_PATH):\n",
    "    print(\"Please download the speaker model from \"\n",
    "        \"https://alphacephei.com/vosk/models and unpack as {SPK_MODEL_PATH} \"\n",
    "        \"in the current folder.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is Speaker Embeddings?\n",
    "An x-vector (or speaker embedding) is a compact numerical representation (a vector of numbers, e.g., [0.12, -0.45, ...]) that captures the unique characteristics of a speaker's voice, such as pitch, tone, and vocal tract shape. You can understand it as \"voice fingerprint\".\n",
    "\n",
    "1. **What It Is**:  \n",
    "   - A **fixed-length vector** (e.g., 512 numbers) extracted from audio using a neural network.  \n",
    "   - Represents **voice identity**, not the spoken words.  \n",
    "   - Example: Two recordings of *different sentences* from the **same speaker** will have **similar x-vectors**.  \n",
    "\n",
    "2. **How It Works**:  \n",
    "   - A model (like VOSK's `spk-model`) analyzes the audio and outputs the x-vector.  \n",
    "   - Computed from **spectral features** (e.g., MFCCs) of the voice.  \n",
    "\n",
    "3. **Purpose**:  \n",
    "   - **Speaker Verification**: Confirm if two audios are from the same person (e.g., voice authentication).  \n",
    "   - **Speaker Diarization**: Label \"who spoke when\" in a conversation.  \n",
    "   - **Compare Speakers**: Measure similarity using **cosine distance** (small distance = similar voices).  \n",
    "\n",
    "4. **X-Vector vs. Raw Audio**:  \n",
    "   | Feature          | Raw Audio (WAV)               | X-Vector                          |  \n",
    "   |------------------|-------------------------------|-----------------------------------|  \n",
    "   | **Format**       | Time-series sound samples     | Compact numerical vector (e.g., 512-dim) |  \n",
    "   | **Content**      | Words + noise + speaker traits | Only speaker traits               |  \n",
    "   | **Usage**        | Playback, ASR                 | Speaker recognition/comparison    |  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##### **Why Use X-Vectors?**  \n",
    "- **Efficiency**: A 5s audio → 512 numbers (easy to store/compare).  \n",
    "- **Privacy**: No raw audio is stored, just the voice fingerprint.  \n",
    "- **Accuracy**: Beats older methods (like i-vectors) for speaker recognition.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opening and Validating the Audio File\n",
    "\n",
    "1. Opens a WAV file for reading in binary mode (\"rb\")\n",
    "2. Validates 3 critical properties of the audio file:\n",
    "    - Must be mono (single channel)\n",
    "    - Must use 16-bit PCM encoding (2 bytes per sample)\n",
    "    - Must be uncompressed (no compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = \"\"\n",
    "\n",
    "wf = wave.open(wav_file, \"rb\")\n",
    "if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "    print(\"Audio file must be WAV format mono PCM.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setting Up Speech and Speaker Recogntion Models\n",
    "\n",
    "- Loads the speech recognition model (en-us for English).\n",
    "- Loads the speaker model for identifying speaker embeddings (x-vectors).\n",
    "- Creates a recognizer (rec) that transcribes speech and detects speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large vocabulary free form recognition\n",
    "model = Model(lang=\"en-us\")\n",
    "spk_model = SpkModel(SPK_MODEL_PATH)\n",
    "#rec = KaldiRecognizer(model, wf.getframerate(), spk_model)\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetSpkModel(spk_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get Speaker Signature\n",
    "\n",
    "[Using this link to get your own voice signature!](./Tools-Get_Voice_Signature.ipynb)\n",
    "\n",
    "1. Record a wav file\n",
    "2. transfer to wav\n",
    "3. using wav file to generate spk_sig variable as numpy array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Module-Vosk-NB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
